{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bec27d86-fec8-4b7f-a1d6-d45c482f7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Dice(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Dice, self).__init__()\n",
    "        self.bn = layers.BatchNormalization(center=False, scale=False)\n",
    "        self.alpha = self.add_weight(shape=(), initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x_normed = self.bn(x)\n",
    "        p = tf.sigmoid(x_normed)\n",
    "        return p * x + (1 - p) * self.alpha * x\n",
    "\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, hidden_units, use_softmax=False):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dense_layer = tf.keras.Sequential()\n",
    "        for units in hidden_units:\n",
    "            self.dense_layer.add(layers.Dense(units, activation='sigmoid'))\n",
    "        self.output_layer = layers.Dense(1)\n",
    "        self.use_softmax = use_softmax\n",
    "\n",
    "    def call(self, query, keys, keys_mask):\n",
    "        # query: [batch_size, embedding_dim]\n",
    "        # keys: [batch_size, max_length, embedding_dim]\n",
    "        # keys_mask: [batch_size, max_length]\n",
    "        query = tf.tile(tf.expand_dims(query, 1), [1, tf.shape(keys)[1], 1])  # [batch_size, max_length, embedding_dim]\n",
    "        inputs = tf.concat([query, keys, query - keys, query * keys], axis=-1)\n",
    "        outputs = self.dense_layer(inputs)\n",
    "        scores = self.output_layer(outputs)  # [batch_size, max_length, 1]\n",
    "        scores = tf.squeeze(scores, axis=-1)  # [batch_size, max_length]\n",
    "\n",
    "        # 应用掩码\n",
    "        paddings = tf.ones_like(scores) * (-2 ** 32 + 1)\n",
    "        scores = tf.where(keys_mask, scores, paddings)\n",
    "\n",
    "        if self.use_softmax:\n",
    "            scores = tf.nn.softmax(scores)  # [batch_size, max_length]\n",
    "        output = tf.reduce_sum(keys * tf.expand_dims(scores, -1), axis=1)  # [batch_size, embedding_dim]\n",
    "        return output\n",
    "\n",
    "\n",
    "class DIN(tf.keras.Model):\n",
    "    def __init__(self, user_feature_dim, item_feature_dim, embedding_dim, hidden_units, attention_hidden_units):\n",
    "        super(DIN, self).__init__()\n",
    "        self.user_embedding = layers.Embedding(user_feature_dim, embedding_dim)\n",
    "        self.item_embedding = layers.Embedding(item_feature_dim, embedding_dim)\n",
    "        self.attention = Attention(attention_hidden_units)\n",
    "        self.dense_layer = tf.keras.Sequential()\n",
    "        for units in hidden_units:\n",
    "            self.dense_layer.add(layers.Dense(units))\n",
    "            self.dense_layer.add(Dice())\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, user_features, item_features, hist_item_features, hist_mask):\n",
    "        user_emb = self.user_embedding(user_features)  # [batch_size, embedding_dim]\n",
    "        item_emb = self.item_embedding(item_features)  # [batch_size, embedding_dim]\n",
    "        hist_item_embs = self.item_embedding(hist_item_features)  # [batch_size, max_length, embedding_dim]\n",
    "\n",
    "        hist_attention_emb = self.attention(item_emb, hist_item_embs, hist_mask)  # [batch_size, embedding_dim]\n",
    "\n",
    "        inputs = tf.concat([user_emb, item_emb, hist_attention_emb], axis=-1)\n",
    "        outputs = self.dense_layer(inputs)\n",
    "        output = self.output_layer(outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "user_feature_dim = 100\n",
    "item_feature_dim = 200\n",
    "embedding_dim = 16\n",
    "hidden_units = [32, 16]\n",
    "attention_hidden_units = [32, 16]\n",
    "\n",
    "model = DIN(user_feature_dim, item_feature_dim, embedding_dim, hidden_units, attention_hidden_units)\n",
    "\n",
    "# 模拟输入数据\n",
    "batch_size = 32\n",
    "user_features = tf.random.uniform([batch_size], minval=0, maxval=user_feature_dim, dtype=tf.int32)\n",
    "item_features = tf.random.uniform([batch_size], minval=0, maxval=item_feature_dim, dtype=tf.int32)\n",
    "max_length = 10\n",
    "hist_item_features = tf.random.uniform([batch_size, max_length], minval=0, maxval=item_feature_dim, dtype=tf.int32)\n",
    "real_length = tf.random.uniform([batch_size], minval=0, maxval=max_length, dtype=tf.int32)\n",
    "hist_mask = tf.sequence_mask(real_length, max_length)\n",
    "\n",
    "# 前向传播\n",
    "output = model(user_features, item_features, hist_item_features, hist_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a3fb181-1838-4792-9fd8-8275fda05a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
